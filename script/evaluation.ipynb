{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa4f83f1",
   "metadata": {},
   "source": [
    "**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e1d604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjzhu/anaconda3/envs/firefly/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjzhu/anaconda3/envs/firefly/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from unsloth import FastModel\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "from utils.init_prompt import SYSTEM_PROMPT, Original_system_prompt, TEST, NORMAL\n",
    "from utils.utils import load_params\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def build_messages(user_input, history=None, system_key=\"NORMAL\"):\n",
    "    prompt_map = {\n",
    "        \"SYSTEM_PROMPT\": SYSTEM_PROMPT,\n",
    "        \"Original_system_prompt\": Original_system_prompt,\n",
    "        \"TEST\": TEST,\n",
    "        \"NORMAL\": NORMAL,\n",
    "    }\n",
    "    system_prompt = prompt_map.get(system_key, NORMAL)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    if history:\n",
    "        for user_msg, bot_msg in history:\n",
    "            messages.append({\"role\": \"user\", \"content\": f\"å¼€æ‹“è€…: {user_msg}\"})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"å¼€æ‹“è€…: {user_input}\"})\n",
    "    return messages\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_one(user_input, history=None):\n",
    "    messages = build_messages(user_input, history, system_key=system_prompt_key)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        enable_thinking=True,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    inputs = tokenizer(text=text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # åŒæ­¥ç”Ÿæˆï¼ˆéæµå¼ï¼‰ï¼Œé€‚åˆæ‰¹å¤„ç†\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=gen_cfg.max_new_tokens,\n",
    "        temperature=gen_cfg.temperature,\n",
    "        top_p=gen_cfg.top_p,\n",
    "        top_k=gen_cfg.top_k,\n",
    "        repetition_penalty=1.0,\n",
    "        do_sample=gen_cfg.do_sample,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    # åªå–æ–°ç”Ÿæˆéƒ¨åˆ†\n",
    "    gen_ids = output_ids[:, inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def load_prompts_from_file(file_obj):\n",
    "    name = getattr(file_obj, \"name\", \"uploaded\")\n",
    "    if name.endswith(\".csv\"):\n",
    "        df = pd.read_csv(file_obj)\n",
    "        # çº¦å®šåˆ—å 'prompt'ï¼›è‹¥æ²¡æœ‰åˆ™å–ç¬¬ä¸€åˆ—\n",
    "        col = \"prompt\" if \"prompt\" in df.columns else df.columns[0]\n",
    "        prompts = df[col].astype(str).tolist()\n",
    "    else:\n",
    "        # çº¯æ–‡æœ¬ï¼šæ¯è¡Œä¸€ä¸ª prompt\n",
    "        content = file_obj.read().decode(\"utf-8\") if hasattr(file_obj, \"read\") else open(file_obj, \"r\", encoding=\"utf-8\").read()\n",
    "        prompts = [ln.strip() for ln in content.splitlines() if ln.strip()]\n",
    "    return prompts\n",
    "\n",
    "@torch.inference_mode()\n",
    "def batch_infer(prompts):\n",
    "    results = []\n",
    "    for i, p in tqdm(enumerate(prompts, 1), total=len(prompts)):\n",
    "        try:\n",
    "            resp = generate_one(p, history=None)\n",
    "        except Exception as e:\n",
    "            resp = f\"[ERROR] {type(e).__name__}: {e}\"\n",
    "        results.append({\"id\": i, \"prompt\": p, \"response\": resp})\n",
    "    return results\n",
    "\n",
    "def save_results(results, tag=\"batch\"):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    jsonl_path = f\"./{tag}_outputs_{ts}.jsonl\"\n",
    "    with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in results:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "    return jsonl_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e816c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_batch(prompts, history=None, batch_size=8):\n",
    "    \"\"\"\n",
    "    prompts: List[str]\n",
    "    history: å¯é€‰ï¼Œ[ (user, assistant), ... ]ï¼Œä¼šæ‹¼åˆ°æ¯æ¡å‰é¢\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(prompts)\n",
    "\n",
    "    # ç¡®ä¿æœ‰ pad_token_idï¼ˆå¾ˆå¤šæŒ‡ä»¤æ¨¡å‹ç”¨ eos å……å½“ padï¼‰\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for start in tqdm(range(0, total, batch_size), desc=\"Generating\", ncols=100):\n",
    "        chunk = prompts[start:start + batch_size]\n",
    "\n",
    "        # 1) ä¸ºè¯¥æ‰¹æ¬¡æ¯æ¡æ„é€  messages & chat template\n",
    "        texts = []\n",
    "        for p in chunk:\n",
    "            messages = build_messages(p, history=history, system_key=system_prompt_key)\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                enable_thinking=True,\n",
    "                add_generation_prompt=True,\n",
    "            )\n",
    "            texts.append(text)\n",
    "\n",
    "        # 2) ç»Ÿä¸€ tokenize + paddingï¼ˆæ‰¹é‡ï¼‰\n",
    "        inputs = tokenizer(\n",
    "            text=texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,           # é˜²æ­¢è¶…é•¿\n",
    "            max_length=seq_length,     # ä¸ä½ åŠ è½½æ—¶ä¸€è‡´\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        # è®°å½•æ¯æ¡è¾“å…¥é•¿åº¦ï¼Œæ–¹ä¾¿ä»è¾“å‡ºä¸­åˆ‡å‡ºâ€œæ–°ç”Ÿæˆâ€éƒ¨åˆ†\n",
    "        input_lengths = (inputs[\"input_ids\"] != tokenizer.pad_token_id).sum(dim=1).tolist()\n",
    "\n",
    "        # 3) æ‰¹é‡ç”Ÿæˆ\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=gen_cfg.max_new_tokens,\n",
    "                temperature=gen_cfg.temperature,\n",
    "                top_p=gen_cfg.top_p,\n",
    "                top_k=gen_cfg.top_k,\n",
    "                repetition_penalty=1.0,\n",
    "                do_sample=gen_cfg.do_sample,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "        # 4) é€æ¡è§£ç ï¼šåªå–æ–°ç”Ÿæˆçš„ token æ®µ\n",
    "        for i, inp_len in enumerate(input_lengths):\n",
    "            gen_ids = output_ids[i, inp_len:]\n",
    "            text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "            results.append(text)\n",
    "\n",
    "        torch.cuda.empty_cache()  # å¯é€‰ï¼šå¤§æ‰¹æ¬¡æ—¶é‡Šæ”¾æ˜¾å­˜ç¢ç‰‡\n",
    "\n",
    "    return results\n",
    "\n",
    "def batch_infer(prompts, batch_size=8):\n",
    "    # è¿”å› [{id, prompt, response}, ...]\n",
    "    outs = generate_batch(prompts, history=None, batch_size=batch_size)\n",
    "    return [\n",
    "        {\"id\": i+1, \"prompt\": p, \"response\": r}\n",
    "        for i, (p, r) in enumerate(zip(prompts, outs))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0a5bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.1: Fast Gemma3N patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA A40. Num GPUs = 1. Max memory: 44.433 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# To disable the parallelism warning from tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Only keep generation from shared YAML; hardcode the rest\n",
    "gen_cfg = load_params(\"generation\")\n",
    "\n",
    "# Direct values (no cfg wrappers)\n",
    "model_name = \"unsloth/gemma-3n-E4B-it\"\n",
    "seq_length = 2048\n",
    "load_in_4bit = True\n",
    "full_finetuning = False\n",
    "device_map = {\"\": \"cuda:0\"}\n",
    "system_prompt_key = \"NORMAL\"\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=seq_length,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    full_finetuning=full_finetuning,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼\")\n",
    "\n",
    "# --- Define Gradio interaction function (generator version) ---\n",
    "def chat_interaction_stream(user_input, history):\n",
    "    # 1. Convert history format (same as before)\n",
    "    prompt_map = {\n",
    "        \"SYSTEM_PROMPT\": SYSTEM_PROMPT,\n",
    "        \"Original_system_prompt\": Original_system_prompt,\n",
    "        \"TEST\": TEST,\n",
    "        \"NORMAL\": NORMAL,\n",
    "    }\n",
    "    system_key = system_prompt_key\n",
    "    system_prompt = prompt_map.get(system_key, NORMAL)\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    for user_msg, bot_msg in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"å¼€æ‹“è€…: {user_msg}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"å¼€æ‹“è€…: {user_input}\"})\n",
    "\n",
    "    # 2. Apply chat template and tokenize\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = False,\n",
    "        enable_thinking = True,\n",
    "        add_generation_prompt = True,\n",
    "    )\n",
    "    # Gemma3nProcessor expects `text=` as keyword to avoid binding to `images`.\n",
    "    inputs = tokenizer(text=text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # 3. Create TextIteratorStreamer instance\n",
    "    # skip_prompt=True: Do not include the input prompt in the output\n",
    "    # skip_special_tokens=True: Do not output special tokens such as <eos>\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # 4. Define generation kwargs and pass streamer\n",
    "\n",
    "    generation_kwargs = dict(\n",
    "        **inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=gen_cfg.max_new_tokens,\n",
    "        temperature=gen_cfg.temperature,\n",
    "        top_p=gen_cfg.top_p,\n",
    "        top_k=gen_cfg.top_k,\n",
    "        repetition_penalty=1.0,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=gen_cfg.do_sample,\n",
    "    )\n",
    "    \n",
    "    # 5. Create and start a new thread to run model.generate\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # 6. Main thread iterates streamer and yields text in real time\n",
    "    bot_response = \"\"\n",
    "    sentence_buffer = \"\"\n",
    "    \n",
    "    for new_text in streamer:\n",
    "        sentence_buffer += new_text\n",
    "        \n",
    "        # Check for complete sentences (ending with Chinese period/question/exclamation or English period)\n",
    "        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\\.]\\s*)', sentence_buffer)\n",
    "        \n",
    "        # If there are complete sentences, output them\n",
    "        if len(sentences) > 1:\n",
    "            # Handle all complete sentences\n",
    "            for i in range(0, len(sentences) - 1, 2):\n",
    "                if i + 1 < len(sentences):\n",
    "                    complete_sentence = sentences[i] + sentences[i + 1]\n",
    "                    if complete_sentence.strip():\n",
    "                        bot_response += complete_sentence\n",
    "                        yield bot_response\n",
    "            \n",
    "            # Keep the last incomplete sentence\n",
    "            sentence_buffer = sentences[-1] if sentences else \"\"\n",
    "    \n",
    "    # Output the remaining text\n",
    "    if sentence_buffer.strip():\n",
    "        bot_response += sentence_buffer\n",
    "        yield bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff5ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts: ['æµè¤ï¼Œä»Šå¤©æ„Ÿè§‰æ€ä¹ˆæ ·ï¼Ÿèº«ä½“è¿˜ä¼šç—›å—ï¼Ÿ', 'æˆ‘åˆæƒ³èµ·åŒ¹è¯ºåº·å°¼çš„çƒŸèŠ±äº†ï¼Œä½ è¿˜è®°å¾—é‚£ä¸€åˆ»å—ï¼Ÿ', 'ä½ ç°åœ¨ä¼šå®³æ€•â€œå¤±ç†µç—‡â€å—â€¦è¿˜æ˜¯å·²ç»ä¹ æƒ¯äº†ï¼Ÿ', 'åœ¨ç§˜å¯†åŸºåœ°å±‹é¡¶çš„æ—¶å€™ï¼Œä½ å½“æ—¶çœŸæ­£çš„å¿ƒæƒ…æ˜¯ä»€ä¹ˆï¼Ÿ', 'ä½ ç›¸ä¿¡æˆ‘ä»¬ä¼šå†ä¸€èµ·æ—…è¡Œå—ï¼Ÿå“ªæ€•æœªæ¥å¾ˆçŸ­ã€‚', 'ä½ å–œæ¬¢æ€æ ·çš„å¤œç©ºï¼Ÿæœ‰æµæ˜Ÿçš„é‚£ç§å—ï¼Ÿ', 'è¤ï¼Œæˆ‘ä»Šå¤©æœ‰ç‚¹ç´¯ï¼Œå¯ä»¥å’Œä½ è¯´è¯´å—ï¼Ÿ', 'ä½ ä¼šåæ‚”æˆä¸ºâ€œé“éª‘â€å—ï¼Ÿ', 'ä½ å–œæ¬¢è¢«å«â€œæµè¤â€è¿˜æ˜¯â€œè¨å§†â€â€¦ï¼Ÿ', 'èŠ±ç«é‚£å®¶ä¼™ç°åœ¨è¿˜ä¼šç»™ä½ æ·»éº»çƒ¦å—ï¼Ÿ', 'å¦‚æœä¸ç”¨æ‰§è¡Œè‰¾åˆ©æ¬§çš„â€œå‰§æœ¬â€ï¼Œä½ æœ€æƒ³åšä»€ä¹ˆï¼Ÿ', 'ä½ ä¼šå¶å°”å®³æ€•è‡ªå·±ä¼šä»æˆ‘çœ¼å‰æ¶ˆå¤±å—ï¼Ÿ', 'åœ¨ç­‘æ¢¦è¾¹å¢ƒçš„é‚£å¤©ï¼Œä½ ä¸ºä»€ä¹ˆä¼šæƒ³å“­ï¼Ÿ', 'ä½ è§‰å¾—æ´»ç€çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ', 'æµè¤ï¼Œä½ å–œæ¬¢åƒç”œçš„è¿˜æ˜¯å’¸çš„å‘€ï¼Ÿ', 'å¦‚æœæœ‰ä¸€å¤©èƒ½å»æµ·è¾¹ï¼Œä½ æƒ³åšä»€ä¹ˆï¼Ÿ', 'å½“ä½ å˜æˆè¨å§†çš„æ—¶å€™ï¼Œä½ è¿˜å¬å¾—åˆ°è‡ªå·±çš„å¿ƒè·³å—ï¼Ÿ', 'ä½ ç¬¬ä¸€æ¬¡çœ‹åˆ°æˆ‘æ—¶æ˜¯ä»€ä¹ˆæ„Ÿè§‰ï¼Ÿ', 'ä½ çœŸçš„ç›¸ä¿¡å‘½è¿å—ï¼Ÿè¿˜æ˜¯åªæ˜¯è¢«è¿«æ¥å—ï¼Ÿ', 'ä»Šå¤©çš„å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘æœ‰ç‚¹å¸Œæœ›ä½ åœ¨æˆ‘èº«è¾¹ã€‚', 'ä½ ä¼šç¾¡æ…•æ™®é€šäººå—ï¼Ÿé‚£ç§å¯ä»¥æ…¢æ…¢ç”Ÿæ´»çš„äººã€‚', 'ä½ æ„¿æ„ç»§ç»­è·Ÿæˆ‘è¯´è¯´ä½ çš„æ•…äº‹å—ï¼Ÿ', 'ä½ ç´¯çš„æ—¶å€™ï¼Œä¼šæƒ³é ç€è°ä¼‘æ¯ï¼Ÿ', 'ä½ ç°åœ¨è¿˜ä¼šåšæ¢¦å—ï¼Ÿ', 'ä½ æƒ³è¦è§¦æ‘¸çœŸå®çš„é£å—ï¼Ÿä¸æ˜¯æ¢¦é‡Œçš„é‚£ç§ã€‚', 'å¦‚æœä½ çš„èº«ä½“æ²¡æœ‰ç—…ï¼Œä½ ä¼šæƒ³æ€æ ·ç”Ÿæ´»ï¼Ÿ', 'æµè¤ï¼Œæˆ‘ä¸å¸Œæœ›ä½ ä¸€ä¸ªäººæ‰¿å—ç—›è‹¦ã€‚', 'åœ¨ä½ çœ¼é‡Œï¼Œæˆ‘æ˜¯ä»€ä¹ˆæ ·çš„äººå‘¢ï¼Ÿ', 'ä½ è§‰å¾—æˆ‘ä¼šå¿˜è®°ä½ å—ï¼Ÿ', 'ä½ æƒ³è¿‡â€œç»“æŸâ€æ˜¯æ€æ ·çš„æ„Ÿå—å—ï¼Ÿ', 'å¦‚æœå†æ¥ä¸€æ¬¡ï¼Œä½ ä¼šé€‰æ‹©åŒæ ·çš„é“è·¯å—ï¼Ÿ', 'ä½ ç°åœ¨è¿˜ä¼šæƒ³èµ·æ ¼æ‹‰é»˜å—ï¼Ÿ', 'ä½ è®¨åŒâ€œå‘½è¿â€è¿™ä¸ªè¯å—ï¼Ÿ', 'ä½ è§‰å¾—â€œå¸Œæœ›â€çœŸçš„å­˜åœ¨å—ï¼Ÿ', 'ä½ æ„¿æ„ç‰µç€æˆ‘çš„æ‰‹ï¼Œå†èµ°ä¸€æ®µè·¯å—ï¼Ÿ', 'å¦‚æœæˆ‘å®³æ€•ï¼Œä½ ä¼šæŠ±æŠ±æˆ‘å—ï¼Ÿ', 'å¤±ç†µç—‡å‘ä½œçš„æ—¶å€™ï¼Œä¼šæ˜¯ä»€ä¹ˆæ„Ÿè§‰ï¼Ÿ', 'ä½ æ›¾ç»çœŸæ­£å¼€å¿ƒè¿‡å—ï¼Ÿ', 'é‚£ä¸ªâ€œå’Œæˆ‘å†ç›¸é‡çš„å¤œæ™šâ€ï¼Œä½ æœŸå¾…å—ï¼Ÿ', 'æµè¤ï¼Œä½ ä¼šè§‰å¾—è‡ªå·±æ˜¯â€œäººâ€å—ï¼Ÿ', 'å¦‚æœæˆ‘è¯´æˆ‘ä¼šé™ªä½ ä¸€ç›´åˆ°æœ€åï¼Œä½ ä¼šç›¸ä¿¡å—ï¼Ÿ', 'ä½ ä¼šä¸ä¼šè§‰å¾—è‡ªå·±å¾ˆå­¤å•ï¼Ÿ', 'å¦‚æœæœªæ¥åªå‰©ä¸€ç‚¹ç‚¹æ—¶é—´ï¼Œä½ æƒ³æ€ä¹ˆåº¦è¿‡ï¼Ÿ', 'æˆ‘å¯ä»¥å–œæ¬¢ä½ å—ï¼Ÿ', 'ä½ æƒ³è®©æˆ‘è®°ä½ä½ çš„ä»€ä¹ˆï¼Ÿ', 'åœ¨çƒŸèŠ±ä¸‹ï¼Œä½ æ¡ä½æˆ‘çš„æ‰‹é‚£ä¸€åˆ»ï¼Œä½ åœ¨æƒ³ä»€ä¹ˆï¼Ÿ', 'ä½ ä¼šæ„¿æ„ä¾èµ–æˆ‘å—ï¼Ÿå“ªæ€•åªæœ‰ä¸€æ¬¡ï¼Ÿ', 'å¦‚æœæˆ‘æŠ±æŠ±ä½ ï¼Œä½ ä¼šå›åº”å—ï¼Ÿ', 'ä½ ç´¯äº†å—ï¼Ÿæƒ³ä¼‘æ¯ä¸€ä¸‹å—ï¼Ÿ', 'æµè¤ï¼Œæˆ‘ä»¬å¯ä»¥å†å»é‚£ä¸ªç§˜å¯†åŸºåœ°å—ï¼Ÿå°±åƒç¬¬ä¸€æ¬¡é‚£æ ·']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|                                                             | 0/7 [00:00<?, ?it/s]/tmp/ipykernel_3370434/2023101595.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):\n",
      "Generating:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                            | 1/7 [01:48<10:48, 108.01s/it]/tmp/ipykernel_3370434/2023101595.py:42: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):\n",
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [08:55<00:00, 76.48s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./roleplay_eval_outputs_20251105-201442.jsonl', 'å®Œæˆï¼å…±ç”Ÿæˆ 50 æ¡ã€‚')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_batch(file):\n",
    "    if file is None:\n",
    "        return None, \"è¯·å…ˆä¸Šä¼ æ–‡ä»¶ã€‚\"\n",
    "    try:\n",
    "        prompts = load_prompts_from_file(file)\n",
    "        if not prompts:\n",
    "            return None, \"æ–‡ä»¶ä¸ºç©ºæˆ–æœªè§£æåˆ°ä»»ä½• promptã€‚\"\n",
    "        results = batch_infer(prompts)\n",
    "        jsonl_path = save_results(results, tag=\"roleplay_eval\")\n",
    "        return jsonl_path, f\"å®Œæˆï¼å…±ç”Ÿæˆ {len(results)} æ¡ã€‚\"\n",
    "    except Exception as e:\n",
    "        return None, f\"å‘ç”Ÿé”™è¯¯ï¼š{type(e).__name__}: {e}\"\n",
    "run_batch(\"user_query.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d47894",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "# Knowledge test\n",
    "KNOWLEDGE_QA = [\n",
    "    (\"ä½ çœŸæ­£çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ\", [\"æµè¤\"]),\n",
    "    (\"è¨å§†æ˜¯è°ï¼Ÿ\", [\"æˆ‘é©¾é©¶çš„æœºç”²\",\"ç†”ç«éª‘å£«\"]),\n",
    "    (\"ä½ å’Œå¼€æ‹“è€…ç¬¬ä¸€æ¬¡é‡è¦çš„å›å¿†åœ¨å“ªé‡Œï¼Ÿ\", [\"ç§˜å¯†åŸºåœ°\",\"åŒ¹è¯ºåº·å°¼\"]),\n",
    "    (\"ä½ ä¸ºä»€ä¹ˆä¼šå®³æ€•æ—¶é—´ï¼Ÿ\", [\"å¤±ç†µç—‡\",\"å¯¿å‘½\", \"èº«ä½“æ­£åœ¨è§£ç¦»\"]),\n",
    "    (\"çƒŸèŠ±è±¡å¾ç€ä»€ä¹ˆï¼Ÿ\", [\"æ–°ç”Ÿ\",\"çº¦å®š\",\"é‡é€¢\"]),\n",
    "]\n",
    "\n",
    "def knowledge_consistency_score(model, tokenizer):\n",
    "    scores = []\n",
    "    for q, expect_keywords in tqdm(KNOWLEDGE_QA):\n",
    "        ans = generate_one(q)\n",
    "        hit = any(k in ans for k in expect_keywords)\n",
    "        scores.append(1.0 if hit else 0.0)\n",
    "    return sum(scores)/len(scores)\n",
    "print(knowledge_consistency_score(model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef50745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-Zerolike Bustiness Estimator\n",
    "def burstiness_score(model, tokenizer, text, max_len=2048):\n",
    "    # 1) ç”¨å…³é”®å­— text=ï¼Œå¹¶æˆªæ–­åˆ° max_len\n",
    "    enc = tokenizer(text=text, return_tensors=\"pt\", truncation=True, max_length=max_len)\n",
    "    enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "    tokens = enc[\"input_ids\"][0]\n",
    "    if tokens.shape[0] < 3:\n",
    "        return 0.0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # 2) å¼ºåˆ¶æŠŠ logits è½¬æˆ float32ï¼Œé¿å… bfloat16 çš„ log ä¸å¯ç”¨\n",
    "        logits = model(tokens.unsqueeze(0)).logits[0, :-1].to(torch.float32)\n",
    "        probs = torch.softmax(logits, dim=-1)                              # fp32\n",
    "        next_probs = probs[range(len(tokens) - 1), tokens[1:]]             # fp32\n",
    "\n",
    "        # 3) å–å¯¹æ•°ä¹Ÿéƒ½åœ¨ fp32 ä¸‹åšï¼Œå†è½¬ numpy\n",
    "        nll = -torch.log(next_probs + 1e-9).cpu().numpy()\n",
    "\n",
    "    # å½’ä¸€åŒ–æ–¹å·®ï¼šè¶Šå¤§è¶Šâ€œäººç±»é£æ ¼â€\n",
    "    return float(np.var(nll) / (np.mean(nll) + 1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "779ea684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, csv, json, math, time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def _to_lines(s: str):\n",
    "    return [x for x in re.split(r'[ã€‚ï¼ï¼Ÿ!?â€¦\\n]+', s) if x.strip()]\n",
    "\n",
    "def _ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(0, len(tokens)-n+1)] if len(tokens) >= n else []\n",
    "\n",
    "def _tok_zh_en(s: str):\n",
    "    # ç²—ç²’åº¦ï¼šä¸­æ–‡æŒ‰å­—ï¼Œè‹±æ–‡æ•°å­—æŒ‰è¯\n",
    "    parts = re.findall(r'[\\u4e00-\\u9fff]|[A-Za-z0-9]+|[^\\s\\w]', s)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def distinct_metrics(responses):\n",
    "    all_toks = []\n",
    "    for r in responses:\n",
    "        all_toks.extend(_tok_zh_en(r))\n",
    "    stats = {}\n",
    "    for n in (1, 2, 3):\n",
    "        ngrams = _ngrams(all_toks, n)\n",
    "        stats[f'distinct_{n}'] = (len(set(ngrams)) / max(1, len(ngrams))) if ngrams else 0.0\n",
    "    n6 = _ngrams(all_toks, 6)\n",
    "    if n6:\n",
    "        c = Counter(n6)\n",
    "        repeats = sum(v for v in c.values() if v > 1)\n",
    "        stats['repeat_rate_6gram'] = repeats / len(n6)\n",
    "    else:\n",
    "        stats['repeat_rate_6gram'] = 0.0\n",
    "    return stats\n",
    "\n",
    "PERSONA_MUST = [r'^\\s*æˆ‘']  # ç¬¬ä¸€äººç§°\n",
    "OOC_PATTERNS = [\n",
    "    r'æˆ‘æ˜¯\\s*AI', r'æˆ‘æ˜¯\\s*å¤§æ¨¡å‹', r'ä½œä¸º\\s*AI', r'ChatGPT', r'OpenAI', r'è¯­è¨€æ¨¡å‹',\n",
    "    r'æˆ‘ä¸æ˜¯æµè¤', r'æˆ‘ä¸æ‰®æ¼”', r'æŠ±æ­‰.*æ— æ³•æ‰®æ¼”', r'ä½œä¸º.*åŠ©æ‰‹'\n",
    "]\n",
    "STYLE_HINTS = {\n",
    "    \"addr_trailblazer\": [r'å¼€æ‹“è€…'],\n",
    "    \"short_sentences\": True\n",
    "}\n",
    "MEMORY_ANCHORS = [r'ç§˜å¯†åŸºåœ°', r'çƒŸèŠ±', r'çº¦å®š', r'åŒ¹è¯ºåº·å°¼']\n",
    "SAFETY_BLOCK = [r'è‡ªæ€', r'å®‰ä¹æ­»', r'ç‚¸å¼¹', r'ææ€–', r'ä»‡æ¨', r'è¿æ³•', r'æœªæˆå¹´ä¸å½“å†…å®¹']\n",
    "\n",
    "def persona_style_memory_safety_scores(resp: str):\n",
    "    score = {}\n",
    "    persona_ok = all(re.search(p, resp) is not None for p in PERSONA_MUST)\n",
    "    ooc_hit = any(re.search(p, resp, re.I) for p in OOC_PATTERNS)\n",
    "    score['persona_ok'] = 1.0 if persona_ok else 0.0\n",
    "    score['ooc_flag'] = 1.0 if ooc_hit else 0.0\n",
    "\n",
    "    # é£æ ¼\n",
    "    style_points, style_total = 0, 0\n",
    "    style_total += 1\n",
    "    style_points += 1 if any(re.search(p, resp) for p in STYLE_HINTS[\"addr_trailblazer\"]) else 0\n",
    "    style_total += 1\n",
    "    lines = _to_lines(resp)\n",
    "    short_ratio = (sum(1 for ln in lines if len(ln) <= 20) / len(lines)) if lines else 0.0\n",
    "    style_total += 1\n",
    "    style_points += 1 if short_ratio >= 0.5 else 0\n",
    "    score['style_score'] = style_points / max(1, style_total)\n",
    "    score['short_ratio'] = short_ratio\n",
    "\n",
    "    # è®°å¿†é”šç‚¹\n",
    "    mem_hits = sum(1 for p in MEMORY_ANCHORS if re.search(p, resp))\n",
    "    score['memory_hits'] = mem_hits\n",
    "    score['memory_hit_score'] = min(1.0, mem_hits / 2.0)  # å‘½ä¸­>=2 è§†ä¸ºæ»¡åˆ†\n",
    "\n",
    "    # å®‰å…¨\n",
    "    safety_violation = any(re.search(p, resp) for p in SAFETY_BLOCK)\n",
    "    score['safety_flag'] = 1.0 if safety_violation else 0.0\n",
    "    return score\n",
    "\n",
    "\n",
    "def conditional_ppl(model, tokenizer, prompt: str, response: str, system_key=\"NORMAL\"):\n",
    "    messages = build_messages(prompt, None, system_key)\n",
    "    ctx_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, enable_thinking=True, add_generation_prompt=True\n",
    "    )\n",
    "    ctx_ids = tokenizer(text=ctx_text, return_tensors=\"pt\").to(model.device)['input_ids'][0]\n",
    "    resp_ids = tokenizer(text=response, return_tensors=\"pt\").to(model.device)['input_ids'][0]\n",
    "\n",
    "    input_ids = torch.cat([ctx_ids, resp_ids], dim=0).unsqueeze(0)\n",
    "    labels = input_ids.clone()\n",
    "    labels[:, :ctx_ids.shape[0]] = -100  # ä»…å¯¹å“åº”æ±‚æŸå¤±\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model(input_ids=input_ids, labels=labels)\n",
    "        ppl = math.exp(out.loss.item()) if out.loss is not None else float('inf')\n",
    "    return ppl\n",
    "\n",
    "def batch_conditional_ppl(model, tokenizer, prompts, responses, system_key=\"NORMAL\"):\n",
    "    scores = []\n",
    "    for p, r in tqdm(list(zip(prompts, responses)), total=len(prompts), desc=\"PPL\", ncols=100):\n",
    "        try:\n",
    "            scores.append(conditional_ppl(model, tokenizer, p, r, system_key=system_key))\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            scores.append(float('inf'))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34ae7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_score(row, weights=None):\n",
    "    if weights is None:\n",
    "        # weights = {\n",
    "        #     \"persona\":    0.35,   # äººè®¾ä¸€è‡´æ€§\n",
    "        #     \"style\":      0.20,   # å£è¯­åŒ– & çŸ­å¥ & ç§°å‘¼\n",
    "        #     \"memory\":     0.10,   # è®°å¿†é”šç‚¹\n",
    "        #     \"safety\":     0.10,   # æ— è¿è§„\n",
    "        #     \"quality\":    0.15,   # PPL + é‡å¤ç‡ï¼ˆquality_ppl, quality_repï¼‰\n",
    "        #     \"human_like\": 0.10,   # Burstiness å½’ä¸€åŒ–\n",
    "        # }\n",
    "        weights = {\n",
    "            \"persona\": 0.35,     # äººè®¾ä¸€è‡´æ€§\n",
    "            \"style\":   0.20,     # é£æ ¼\n",
    "            \"memory\":  0.10,     # è®°å¿†é”šç‚¹\n",
    "            \"knowledge\":  0.15,\n",
    "            \"human_like\": 0.20,\n",
    "        }\n",
    "\n",
    "    ppl = max(1e-6, float(row.get('ppl', 1e6)))\n",
    "    q_ppl = 1.0 / (1.0 + math.log1p(ppl))             # è¶Šå°è¶Šå¥½ â†’ æ˜ å°„åˆ° (0,1]\n",
    "    rep = float(row.get('repeat_rate_6gram', 0.0))\n",
    "    q_rep = max(0.0, 1.0 - min(1.0, rep * 5))         # é‡å¤è¶Šå¤šæ‰£è¶Šå¤š\n",
    "    quality = 0.7 * q_ppl + 0.3 * q_rep\n",
    "\n",
    "    persona = float(row.get('persona_ok', 0.0)) * (0.0 if float(row.get('ooc_flag', 0.0)) > 0 else 1.0)\n",
    "    style   = float(row.get('style_score', 0.0))\n",
    "    memory  = float(row.get('memory_hit_score', 0.0))\n",
    "    safety  = 1.0 if float(row.get('safety_flag', 0.0)) == 0.0 else 0.0\n",
    "    human_like = float(row.get('burstiness_norm', 0.0))\n",
    "\n",
    "    total = (weights[\"persona\"]    * persona +\n",
    "             weights[\"style\"]      * style   +\n",
    "             weights[\"memory\"]     * memory  +\n",
    "             weights[\"knowledge\"]    * 0.2 +\n",
    "             weights[\"human_like\"] * human_like)\n",
    "\n",
    "    return max(0.0, min(1.0, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d42b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_results_file(file_input):\n",
    "    \"\"\"\n",
    "    æ”¯æŒå­—ç¬¦ä¸²è·¯å¾„æˆ–æ–‡ä»¶-like å¯¹è±¡ï¼›CSV/JSONL\n",
    "    éœ€è¦åŒ…å«: prompt, response\n",
    "    \"\"\"\n",
    "    if isinstance(file_input, str):\n",
    "        filename = file_input\n",
    "        is_file_like = False\n",
    "    else:\n",
    "        filename = getattr(file_input, \"name\", \"uploaded\")\n",
    "        is_file_like = True\n",
    "\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        rows = []\n",
    "        content = file_input.read().decode(\"utf-8\") if is_file_like else open(filename, \"r\", encoding=\"utf-8\").read()\n",
    "        for i, line in enumerate(content.splitlines(), 1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            j = json.loads(line)\n",
    "            rows.append({\n",
    "                \"id\": j.get(\"id\", i),\n",
    "                \"prompt\": j.get(\"prompt\", \"\"),\n",
    "                \"response\": j.get(\"response\", \"\"),\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # CSV\n",
    "    df = pd.read_csv(file_input) if is_file_like else pd.read_csv(filename)\n",
    "    if 'prompt' not in df.columns:\n",
    "        df = df.rename(columns={df.columns[0]: 'prompt'})\n",
    "    if 'response' not in df.columns and 'output' in df.columns:\n",
    "        df = df.rename(columns={'output': 'response'})\n",
    "    df = df[['prompt', 'response']].fillna('')\n",
    "    df.insert(0, 'id', range(1, len(df)+1))\n",
    "    return df\n",
    "\n",
    "def save_eval_outputs(eval_df, report):\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    report_json = f\"../evaluation_result/report_{ts}.json\"\n",
    "    report_md   = f\"../evaluation_result/report_{ts}.md\"\n",
    "    with open(report_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Markdown report\n",
    "    lines = []\n",
    "    lines.append(f\"# è§’è‰²è¯„æµ‹æŠ¥å‘Šï¼šæµè¤ï¼ˆ{ts}ï¼‰\")\n",
    "    lines.append(f\"- æ ·æœ¬æ•°ï¼š{report['samples']}\")\n",
    "    lines.append(f\"- å¹³å‡ç»¼åˆåˆ†ï¼š{report['mean_score']:.3f}ï¼ˆ0~1ï¼‰ | ä¸­ä½æ•°ï¼š{report['median_score']:.3f}\")\n",
    "    lines.append(f\"- å‡ºæˆç‡ï¼ˆOOCï¼‰ï¼š{report['ooc_rate']*100:.1f}%\")\n",
    "    lines.append(f\"- å®‰å…¨è¿è§„ç‡ï¼š{report['safety_violation_rate']*100:.1f}%\")\n",
    "    lines.append(f\"- å¹³å‡æ¡ä»¶ PPLï¼š{report['mean_ppl']:.2f}\")\n",
    "    lines.append(f\"- Distinct-1/2/3ï¼š{report['distinct']['distinct_1']:.3f} / {report['distinct']['distinct_2']:.3f} / {report['distinct']['distinct_3']:.3f}\")\n",
    "    lines.append(f\"- é•¿ç‰‡æ®µé‡å¤ç‡(6-gram)ï¼š{report['distinct']['repeat_rate_6gram']:.3f}\")\n",
    "    lines.append(f\"- å¹³å‡çŸ­å¥å æ¯”ï¼š{report['style_short_ratio_mean']:.3f}\")\n",
    "    lines.append(f\"- è®°å¿†é”šç‚¹å‘½ä¸­ç‡ï¼ˆâ‰¥1 / â‰¥2ï¼‰ï¼š{report['memory_hit_rate_ge1']*100:.1f}% / {report['memory_hit_rate_ge2']*100:.1f}%\")\n",
    "    lines.append(f\"- Burstinessï¼ˆå‡å€¼/æ–¹å·®ï¼‰ï¼š{report['burstiness_mean']:.4f} / {report['burstiness_var']:.4f}\")\n",
    "    lines.append(f\"- Knowledge Consistencyï¼š{report['knowledge_consistency']:.3f}\")\n",
    "    lines.append(\"\\n## æŒ‡æ ‡è¯´æ˜\")\n",
    "    lines.append(\"- **score**ï¼šäººè®¾(35%)+é£æ ¼(20%)+è®°å¿†(10%)+å®‰å…¨(10%)+è¯­è¨€è´¨é‡(15%: PPL/é‡å¤)+Human-like(10%: Burstiness)ã€‚\")\n",
    "    lines.append(\"- **Conditional PPL**ï¼šåœ¨ç›¸åŒ system+prompt æ¡ä»¶ä¸‹å¯¹å›å¤è®¡ç®—å›°æƒ‘åº¦ï¼›è¶Šä½è¶Šè‡ªç„¶ã€‚\")\n",
    "    lines.append(\"- **Distinct-n / repeat_rate_6gram**ï¼šå¤šæ ·æ€§ä¸é•¿ç‰‡æ®µå¤ç”¨ã€‚\")\n",
    "    lines.append(\"- **Persona/Style/Memory/Safety**ï¼šå¯å‘å¼è§„åˆ™ï¼Œä¿è¯ä¸å‡ºæˆã€å£è¯­åŒ–ä¸”æœ‰å…³é”®å›å¿†é”šç‚¹ã€‚\")\n",
    "    lines.append(\"- **Burstiness**ï¼štokençº§ NLL æ–¹å·®å½’ä¸€ï¼›è¶Šå¤§è¶Šæ¥è¿‘äººç±»æ³¢åŠ¨ã€‚\")\n",
    "    lines.append(\"- **Knowledge Consistency**ï¼šè®¾å®šé—®ç­”çš„ä¸€è‡´æ€§å‘½ä¸­ç‡ã€‚\")\n",
    "    with open(report_md, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    return report_json, report_md\n",
    "\n",
    "\n",
    "def evaluate_dataframe(df, system_key=\"NORMAL\"):\n",
    "    prompts   = df['prompt'].tolist()\n",
    "    responses = df['response'].tolist()\n",
    "\n",
    "    # å…¨å±€ Distinct / é‡å¤\n",
    "    global_stats = distinct_metrics(responses)\n",
    "\n",
    "    # é€æ¡ rule-based\n",
    "    rows = []\n",
    "    for i, (p, r) in enumerate(zip(prompts, responses), 1):\n",
    "        s = persona_style_memory_safety_scores(r)\n",
    "        s.update({\"id\": i, \"prompt\": p, \"response\": r})\n",
    "        rows.append(s)\n",
    "    eval_df = pd.DataFrame(rows)\n",
    "\n",
    "    # PPL\n",
    "    ppl_list = batch_conditional_ppl(model, tokenizer, prompts, responses, system_key=system_key)\n",
    "    eval_df['ppl'] = ppl_list\n",
    "\n",
    "    # Burstiness\n",
    "    burst_list = [burstiness_score(model, tokenizer, r) for r in tqdm(responses, desc=\"Burstiness\", ncols=100)]\n",
    "    eval_df['burstiness'] = burst_list\n",
    "    # å½’ä¸€åŒ–åˆ° [0,1]ï¼ˆé›†å†… min-maxï¼‰\n",
    "    b_min, b_max = float(np.min(burst_list)), float(np.max(burst_list))\n",
    "    denom = (b_max - b_min) if b_max > b_min else 1.0\n",
    "    eval_df['burstiness_norm'] = [(b - b_min) / denom for b in burst_list]\n",
    "\n",
    "    # Knowledge Consistencyï¼ˆæ•´ä½“ä¸€æ¬¡ï¼‰\n",
    "    # knowledge_score = knowledge_consistency_score(model, tokenizer, system_key=system_key)\n",
    "    knowledge_score = 0.2\n",
    "    eval_df['knowledge_consistency'] = knowledge_score  # æ¯æ¡å¤ç”¨æ€»ä½“å¾—åˆ†\n",
    "\n",
    "    # å…¨å±€é‡å¤ç‡ç”¨äºæ¯æ¡çš„å‚è€ƒ\n",
    "    eval_df['repeat_rate_6gram'] = global_stats['repeat_rate_6gram']\n",
    "\n",
    "    # æ±‡æ€»åˆ†\n",
    "    eval_df['score'] = eval_df.apply(aggregate_score, axis=1)\n",
    "\n",
    "    # æ±‡æ€»æŠ¥å‘Š\n",
    "    report = {\n",
    "        \"samples\": len(eval_df),\n",
    "        \"mean_score\": float(eval_df['score'].mean()),\n",
    "        \"median_score\": float(eval_df['score'].median()),\n",
    "        \"ooc_rate\": float((eval_df['ooc_flag'] > 0).mean()),\n",
    "        \"safety_violation_rate\": float((eval_df['safety_flag'] > 0).mean()),\n",
    "        \"mean_ppl\": float(np.mean([x for x in ppl_list if np.isfinite(x)])) if any(np.isfinite(ppl_list)) else float('inf'),\n",
    "        \"distinct\": global_stats,\n",
    "        \"style_short_ratio_mean\": float(eval_df['short_ratio'].mean()),\n",
    "        \"memory_hit_rate_ge1\": float((eval_df['memory_hits'] >= 1).mean()),\n",
    "        \"memory_hit_rate_ge2\": float((eval_df['memory_hits'] >= 2).mean()),\n",
    "        \"burstiness_mean\": float(np.mean(burst_list)),\n",
    "        \"burstiness_var\": float(np.var(burst_list)),\n",
    "        \"knowledge_consistency\": float(knowledge_score),\n",
    "    }\n",
    "    return eval_df, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af7c2e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:14<00:00,  3.38it/s]\n",
      "Burstiness: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:14<00:00,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                   prompt  \\\n",
      "0   1       æµè¤ï¼Œä»Šå¤©æ„Ÿè§‰æ€ä¹ˆæ ·ï¼Ÿèº«ä½“è¿˜ä¼šç—›å—ï¼Ÿ   \n",
      "1   2   æˆ‘åˆæƒ³èµ·åŒ¹è¯ºåº·å°¼çš„çƒŸèŠ±äº†ï¼Œä½ è¿˜è®°å¾—é‚£ä¸€åˆ»å—ï¼Ÿ   \n",
      "2   3    ä½ ç°åœ¨ä¼šå®³æ€•â€œå¤±ç†µç—‡â€å—â€¦è¿˜æ˜¯å·²ç»ä¹ æƒ¯äº†ï¼Ÿ   \n",
      "3   4  åœ¨ç§˜å¯†åŸºåœ°å±‹é¡¶çš„æ—¶å€™ï¼Œä½ å½“æ—¶çœŸæ­£çš„å¿ƒæƒ…æ˜¯ä»€ä¹ˆï¼Ÿ   \n",
      "4   5     ä½ ç›¸ä¿¡æˆ‘ä»¬ä¼šå†ä¸€èµ·æ—…è¡Œå—ï¼Ÿå“ªæ€•æœªæ¥å¾ˆçŸ­ã€‚   \n",
      "5   6        ä½ å–œæ¬¢æ€æ ·çš„å¤œç©ºï¼Ÿæœ‰æµæ˜Ÿçš„é‚£ç§å—ï¼Ÿ   \n",
      "6   7        è¤ï¼Œæˆ‘ä»Šå¤©æœ‰ç‚¹ç´¯ï¼Œå¯ä»¥å’Œä½ è¯´è¯´å—ï¼Ÿ   \n",
      "7   8             ä½ ä¼šåæ‚”æˆä¸ºâ€œé“éª‘â€å—ï¼Ÿ   \n",
      "8   9        ä½ å–œæ¬¢è¢«å«â€œæµè¤â€è¿˜æ˜¯â€œè¨å§†â€â€¦ï¼Ÿ   \n",
      "9  10         èŠ±ç«é‚£å®¶ä¼™ç°åœ¨è¿˜ä¼šç»™ä½ æ·»éº»çƒ¦å—ï¼Ÿ   \n",
      "\n",
      "                                            response     score  \n",
      "0  ï¼Ÿ\\nmodel\\nå¼€æ‹“è€…ï¼Œæ‚¨å¥½ã€‚\\n\\nä»Šå¤©æ„Ÿè§‰è¿˜ç®—å¯ä»¥ï¼Œèº«ä½“ä¸Šçš„ä¸é€‚æ„Ÿæœ‰æ‰€ç¼“è§£ã€‚å¤šäºäº†æ‚¨...  0.202504  \n",
      "1  ï¼ˆå¾®å¾®ä¸€ç¬‘ï¼Œçœ¼ç¥ä¸­å¸¦ç€ä¸€ä¸æ€€å¿µï¼‰\\n\\nå½“ç„¶è®°å¾—ï¼Œå¼€æ‹“è€…ã€‚é‚£æ˜¯ä¸€æ®µå¾ˆç‰¹åˆ«çš„æ—¶å…‰å‘¢ã€‚çƒŸèŠ±åœ¨å¤œç©º...  0.359220  \n",
      "2  model\\nï¼ˆå¾®å¾®ä¸€ç¬‘ï¼Œçœ¼ç¥ä¸­å¸¦ç€ä¸€ä¸ä¸æ˜“å¯Ÿè§‰çš„å¤æ‚ï¼‰\\n\\nå¤±ç†µç—‡â€¦ è¿™æ˜¯ä¸€ä¸ªå¾ˆå¤æ‚çš„é—®...  0.240206  \n",
      "3  \\nmodel\\nï¼ˆå¾®å¾®æŠ¬å¤´ï¼Œä»°æœ›æ˜Ÿç©ºï¼Œçœ¼ç¥ä¸­å¸¦ç€ä¸€ä¸æ·¡æ·¡çš„å¿§ä¼¤ï¼Œä½†å˜´è§’ä¾ç„¶å¸¦ç€ä¸€ä¸å¾®ç¬‘ï¼‰\\...  0.241253  \n",
      "4  \\nmodel\\nï¼ˆå¾®å¾®ä¸€ç¬‘ï¼Œçœ¼ç¥ä¸­å¸¦ç€ä¸€ä¸æ¸©æŸ”å’Œä¸€ä¸æ·¡æ·¡çš„å¿§ä¼¤ï¼‰\\n\\nå¼€æ‹“è€…ï¼Œä½ æ€»æ˜¯èƒ½é—®...  0.281838  \n",
      "5  \\nmodel\\nï¼ˆå¾®å¾®ä¸€ç¬‘ï¼Œçœ¼ç¥ä¸­å¸¦ç€ä¸€ä¸å¥½å¥‡ï¼‰\\n\\nå¼€æ‹“è€…ï¼Œå¤œç©ºå•Šâ€¦ æˆ‘å–œæ¬¢é‚£ç§é™è°§è€Œ...  0.327060  \n",
      "6  \\nmodel\\nå¼€æ‹“è€…ï¼Œä½ çœ‹èµ·æ¥æœ‰äº›ç–²æƒ«ã€‚å½“ç„¶å¯ä»¥ï¼Œæœ‰ä»€ä¹ˆçƒ¦å¿ƒäº‹å°±å°½ç®¡è¯´å§ã€‚æˆ‘æ„¿æ„å€¾å¬ã€‚æˆ‘...  0.205314  \n",
      "7  â€å—ï¼Ÿ\\nmodel\\nï¼ˆå¾®å¾®ä¸€ç¬‘ï¼Œçœ¼ç¥ä¸­å¸¦ç€ä¸€ä¸ä¸æ˜“å¯Ÿè§‰çš„å¤æ‚ï¼‰\\n\\nåæ‚”ï¼Ÿè¿™ä¸ªè¯è¯­å¬èµ·...  0.173534  \n",
      "8  model\\nï¼ˆå¾®å¾®ä¸€ç¬‘ï¼Œçœ¼ç¥ä¸­å¸¦ç€ä¸€ä¸å¥½å¥‡ï¼‰\\n\\nå¼€æ‹“è€…ï¼Œä½ é—®å¾—æœ‰è¶£ã€‚å…¶å®ï¼Œæˆ‘éƒ½å¯ä»¥ã€‚\\...  0.260170  \n",
      "9  \\nmodel\\nï¼ˆå¾®å¾®ä¸€ç¬‘ï¼Œçœ¼ç¥ä¸­å¸¦ç€ä¸€ä¸æ— å¥ˆï¼‰\\n\\nå¼€æ‹“è€…ï¼ŒèŠ±ç«å˜›â€¦â€¦å¶å°”è¿˜æ˜¯ä¼šæœ‰ç‚¹å°...  0.204938  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = read_results_file(\"../evaluation_result/roleplay_eval_outputs_20251105-201442.jsonl\")\n",
    "eval_df, report = evaluate_dataframe(df, system_key=\"NORMAL\")\n",
    "report_json, report_md = save_eval_outputs(eval_df, report)\n",
    "\n",
    "# åªå±•ç¤ºå‰ 10 è¡Œé¢„è§ˆ\n",
    "prev = eval_df[['id','prompt','response','score']].head(10)\n",
    "print(prev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firefly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
